{"cells":[{"cell_type":"markdown","metadata":{},"source":["### **Ridge Regression Gradient Descent**"]},{"cell_type":"markdown","metadata":{},"source":["Ridge regression is a linear regression method that adds a penalty term to the ordinary least squares (OLS) loss function. This penalty term discourages overly complex models by penalizing large coefficients. The ridge regression cost function can be defined as:\n","\n","\\[ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\]\n","\n","Where:\n","- \\( J(\\theta) \\) is the cost function.\n","- \\( m \\) is the number of training examples.\n","- \\( h_\\theta(x^{(i)}) \\) is the hypothesis function.\n","- \\( y^{(i)} \\) is the actual output for the ith training example.\n","- \\( \\theta_j \\) represents the jth model parameter (or coefficient).\n","- \\( \\lambda \\) is the regularization parameter (also known as alpha) which controls the strength of the regularization. A higher \\( \\lambda \\) leads to a stronger regularization.\n","\n","Gradient descent is an iterative optimization algorithm used for finding the minimum of a function. To minimize the ridge regression cost function, you can use gradient descent. The gradient descent update rule for ridge regression is:\n","\n","\\[ \\theta_j := \\theta_j - \\alpha \\left( \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} + \\frac{\\lambda}{m}\\theta_j \\right) \\]\n","\n","Where:\n","- \\( \\alpha \\) is the learning rate, determining the size of the steps in each iteration.\n","\n","In the update rule, the first term (\\( \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\)) is the derivative of the OLS loss function, and the second term (\\( \\frac{\\lambda}{m}\\theta_j \\)) is the derivative of the regularization term. By simultaneously updating all the \\( \\theta_j \\) values using this rule, you can iteratively converge towards the optimal values that minimize the ridge regression cost function.\n","\n","It's important to normalize the features before applying ridge regression to ensure that all features are on a similar scale, preventing some features from dominating the regularization term unfairly. Also, the choice of the regularization parameter (\\( \\lambda \\)) is crucial and is often determined using techniques like cross-validation."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T14:07:52.855438Z","iopub.status.busy":"2023-10-04T14:07:52.854979Z","iopub.status.idle":"2023-10-04T14:07:52.859706Z","shell.execute_reply":"2023-10-04T14:07:52.858125Z","shell.execute_reply.started":"2023-10-04T14:07:52.855417Z"},"trusted":true},"outputs":[],"source":["# using sci-kit learn library\n","import numpy as np\n","from sklearn.datasets import load_diabetes\n","from sklearn.metrics import r2_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import SGDRegressor\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T14:19:39.238214Z","iopub.status.busy":"2023-10-04T14:19:39.237910Z","iopub.status.idle":"2023-10-04T14:19:39.248921Z","shell.execute_reply":"2023-10-04T14:19:39.247641Z","shell.execute_reply.started":"2023-10-04T14:19:39.238194Z"},"trusted":true},"outputs":[],"source":["X, y = load_diabetes(return_X_y=True)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=4)\n","reg = SGDRegressor(penalty='l2', max_iter=500, eta0=0.1,\n","                   learning_rate='constant', alpha=0.001)\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T14:19:42.977950Z","iopub.status.busy":"2023-10-04T14:19:42.977616Z","iopub.status.idle":"2023-10-04T14:19:42.991336Z","shell.execute_reply":"2023-10-04T14:19:42.990152Z","shell.execute_reply.started":"2023-10-04T14:19:42.977929Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["R2_score: 0.39776487428979046\n","[  51.86761198 -136.85378032  351.3687856   262.28843363   -2.16367803\n","  -52.81062271 -170.44594101  138.81965823  317.79207598  103.88778649]\n","[138.26603412]\n"]}],"source":["reg.fit(X_train, y_train)\n","y_pred = reg.predict(X_test)\n","print(\"R2_score:\", r2_score(y_test, y_pred))\n","print(reg.coef_)\n","print(reg.intercept_)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T14:22:50.587448Z","iopub.status.busy":"2023-10-04T14:22:50.587128Z","iopub.status.idle":"2023-10-04T14:22:50.592397Z","shell.execute_reply":"2023-10-04T14:22:50.591282Z","shell.execute_reply.started":"2023-10-04T14:22:50.587425Z"},"trusted":true},"outputs":[],"source":["# using Ridge Regression for Gradient Descent\n","from sklearn.linear_model import Ridge\n","reg = Ridge(alpha=0.001, max_iter=500, solver='sparse_cg')\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T14:24:31.538327Z","iopub.status.busy":"2023-10-04T14:24:31.537952Z","iopub.status.idle":"2023-10-04T14:24:31.550447Z","shell.execute_reply":"2023-10-04T14:24:31.548966Z","shell.execute_reply.started":"2023-10-04T14:24:31.538299Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["R2_score: 0.4625010162027918\n","[  34.52192778 -290.84083871  482.40181675  368.06786931 -852.44872818\n","  501.59160694  180.11115474  270.76334443  759.73534802   37.49135796]\n","151.101985182554\n"]}],"source":["reg.fit(X_train, y_train)\n","y_pred = reg.predict(X_test)\n","print(\"R2_score:\", r2_score(y_test, y_pred))\n","print(reg.coef_)\n","print(reg.intercept_)\n"]},{"cell_type":"markdown","metadata":{},"source":["# **Ridge Regression Gradient Descent - Scratch Code**"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T15:03:55.866980Z","iopub.status.busy":"2023-10-04T15:03:55.866637Z","iopub.status.idle":"2023-10-04T15:03:55.874361Z","shell.execute_reply":"2023-10-04T15:03:55.872961Z","shell.execute_reply.started":"2023-10-04T15:03:55.866961Z"},"trusted":true},"outputs":[],"source":["class GDRidgeRegression:\n","\n","    def __init__(self, epochs, learning_rate, alpha):\n","\n","        self.learning_rate = learning_rate\n","        self.epochs = epochs\n","        self.alpha = alpha\n","        self.coef_ = None\n","        self.intercept_ = None\n","\n","    def fit(self, X_train, y_train):\n","\n","        self.coef_ = np.ones(X_train.shape[1])\n","        self.intercept_ = 0\n","        thetha = np.insert(self.coef_, 0, self.intercept_)\n","\n","        X_train = np.insert(X_train, 0, 1, axis=1)\n","\n","        for i in range(self.epochs):\n","            thetha_der = np.dot(X_train.T, X_train).dot(\n","                thetha) - np.dot(X_train.T, y_train) + self.alpha*thetha\n","            thetha = thetha - self.learning_rate*thetha_der\n","\n","        self.coef_ = thetha[1:]\n","        self.intercept_ = thetha[0]\n","\n","    def predict(self, X_test):\n","\n","        return np.dot(X_test, self.coef_) + self.intercept_\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-10-04T15:04:00.062003Z","iopub.status.busy":"2023-10-04T15:04:00.061632Z","iopub.status.idle":"2023-10-04T15:04:00.080355Z","shell.execute_reply":"2023-10-04T15:04:00.078948Z","shell.execute_reply.started":"2023-10-04T15:04:00.061977Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["R2 score 0.4738018280260915\n","[  46.65050914 -221.3750037   452.12080647  325.54248128  -29.09464178\n","  -96.47517735 -190.90017011  146.32900372  400.80267299   95.09048094]\n","150.8697531671347\n"]}],"source":["reg = GDRidgeRegression(epochs=500, alpha=0.001, learning_rate=0.005)\n","reg.fit(X_train, y_train)\n","\n","y_pred = reg.predict(X_test)\n","print(\"R2 score\", r2_score(y_test, y_pred))\n","print(reg.coef_)\n","print(reg.intercept_)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
