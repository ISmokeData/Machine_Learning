{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = 'color:green'> **Assignment** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Ans- Lasso Regression is a powerful regression technique that can be used for feature selection and regularization, setting some coefficients to zero, and thus effectively reducing the number of features in the model. It is particularly useful when dealing with high-dimensional data and a large number of features, as it can simplify the model and improve its performance.\n",
    "- Ridge Regression uses L2 regularization, which adds the sum of squared coefficients to the cost function, while Lasso uses L1 regularization, adding the sum of absolute values of coefficients.\n",
    "\n",
    "- Elastic Net combines both L1 and L2 regularization in its cost function, whereas Lasso only uses L1 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "Ans- the main advantage of using Lasso Regression in feature selection is its ability to automatically perform variable selection by setting some coefficients to zero, which leads to a simpler, more interpretable model, better generalization performance, and the identification of relevant predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "Ans-  the interpretation of coefficients in any regression model, including Lasso Regression, should be done carefully and in context with the data and domain knowledge. Additionally, when interpreting the coefficients, it's often helpful to scale the features to have similar ranges, as Lasso regularization is sensitive to the scales of the variables. Standardizing the features (subtracting the mean and dividing by the standard deviation) before fitting the Lasso Regression model can help with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "Ans- The regularization parameter (lambda or alpha) controls the strength of L1 regularization in the Lasso Regression model.\n",
    "- A smaller value of lambda will result in weaker regularization, allowing more coefficients to remain non-zero. This may lead to a model with more features, potentially increasing the risk of overfitting if the number of features is large relative to the number of samples.\n",
    "- On the other hand, a larger value of lambda will lead to stronger regularization, encouraging more coefficients to be exactly zero. This performs more aggressive feature selection and simplifies the model by excluding less important features. It helps in reducing overfitting and improving the model's generalization performance.\n",
    "- The optimal value of lambda is typically determined through techniques like cross-validation. Cross-validation involves dividing the dataset into multiple subsets, training the model on different combinations of these subsets, and evaluating the model's performance. The value of lambda that results in the best performance (e.g., lowest mean squared error or highest R-squared) on the validation set is chosen as the optimal tuning parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "Ans- Feature Engineering:\n",
    "One way to apply Lasso Regression to non-linear regression problems is to manually create new features that capture non-linear relationships between the original features and the target variable. For example, if there is a non-linear relationship between feature X and the target variable Y, you can create new features such as X^2, X^3, sqrt(X), log(X), etc. Then, you can use Lasso Regression with these new features, which introduces non-linear terms into the model.\n",
    "\n",
    "By including these non-linear features in the Lasso Regression model, the model can learn non-linear patterns in the data and still perform feature selection through the L1 regularization. However, this approach requires domain knowledge and understanding of the underlying relationships between the features and the target variable.\n",
    "\n",
    "- Polynomial Regression with Lasso:\n",
    "Polynomial Regression is a type of non-linear regression where the input features are raised to different powers to capture non-linear relationships. You can apply Lasso Regression on top of Polynomial Regression to perform both feature selection and handle non-linearities. In this case, the original features are raised to different powers to create polynomial features, and then Lasso Regression is applied to select the most relevant polynomial features.\n",
    "\n",
    "- Kernel Trick:\n",
    "Another way to handle non-linear regression with Lasso is by using the kernel trick. Kernel methods are popular in support vector machines (SVM) and can be adapted for Lasso Regression as well. The idea is to transform the original feature space into a higher-dimensional space, where non-linear relationships may become linear. This transformation is achieved through the use of kernel functions, such as radial basis function (RBF) kernel.\n",
    "\n",
    "By transforming the data into a higher-dimensional space, you can then apply Lasso Regression in this new feature space to capture non-linear patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "Ans-  the main difference between Ridge Regression and Lasso Regression lies in the type of regularization they use and how they handle the coefficients. Ridge reduces the impact of all coefficients but retains all features, while Lasso performs both regularization and feature selection, leading to a sparser model with some coefficients set to zero. The choice between Ridge and Lasso depends on the specific characteristics of the data and the goal of the analysis, such as whether feature selection is desired or multicollinearity needs to be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "Ans- Yes, Lasso Regression can handle multicollinearity in the input features. Multicollinearity occurs when two or more input features are highly correlated with each other, which can cause instability and unreliable estimates in linear regression models. Lasso Regression addresses multicollinearity through the L1 regularization term in its cost function. Here's how Lasso handles multicollinearity:\n",
    "\n",
    "Feature Selection: Lasso Regression performs feature selection by adding the sum of the absolute values of coefficients (L1 norm) to the cost function. During the optimization process, Lasso encourages some coefficients to become exactly zero. When two or more features are highly correlated (multicollinear), Lasso tends to select one feature over the others and set the coefficients of the less important features to zero. By setting some coefficients to zero, Lasso effectively excludes certain features from the model, reducing multicollinearity issues.\n",
    "\n",
    "Shrinking Coefficients: Lasso also shrinks the coefficients of the remaining features towards zero, which helps reduce the impact of correlated features on the model. By reducing the magnitude of coefficients, Lasso makes the model more robust to multicollinearity, leading to more stable and reliable estimates.\n",
    "\n",
    "Simplicity and Parsimony: Lasso's feature selection capability results in a simpler and more interpretable model. It automatically selects the most relevant features and discards less important or redundant ones, leading to a more parsimonious representation of the relationship between the features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "Ans- The optimal value of the regularization parameter (lambda) in Lasso Regression is typically chosen through techniques like cross-validation, where different values of lambda are tested, and the one that results in the best model performance on a validation dataset is selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
